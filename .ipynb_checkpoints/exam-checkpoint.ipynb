{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 다음과 같은 어레이를 Numpy를 이용하여 만드시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 5, 0.5, float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1, 11, dtype=np.int).reshape(-1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((4, 4), dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5,  7],\n",
       "       [ 9, 11, 13, 15],\n",
       "       [17, 19, 21, 23]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1, 24, 2, dtype=np.int).reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 다음과 같은 데이터프레임을 만드시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>지역</th>\n",
       "      <th>2015</th>\n",
       "      <th>2010</th>\n",
       "      <th>2005</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>서울</th>\n",
       "      <td>수도권</td>\n",
       "      <td>9904312</td>\n",
       "      <td>9631482</td>\n",
       "      <td>9762546</td>\n",
       "      <td>9853972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>부산</th>\n",
       "      <td>경상권</td>\n",
       "      <td>3448737</td>\n",
       "      <td>3393191</td>\n",
       "      <td>3512547</td>\n",
       "      <td>3655437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인천</th>\n",
       "      <td>수도권</td>\n",
       "      <td>2890451</td>\n",
       "      <td>2632035</td>\n",
       "      <td>2517680</td>\n",
       "      <td>2466338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>대구</th>\n",
       "      <td>경상권</td>\n",
       "      <td>2466052</td>\n",
       "      <td>2431774</td>\n",
       "      <td>2456016</td>\n",
       "      <td>2473990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     지역     2015     2010     2005     2000\n",
       "서울  수도권  9904312  9631482  9762546  9853972\n",
       "부산  경상권  3448737  3393191  3512547  3655437\n",
       "인천  수도권  2890451  2632035  2517680  2466338\n",
       "대구  경상권  2466052  2431774  2456016  2473990"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    '지역': ['수도권', '경상권', '수도권', '경상권'],\n",
    "    '2015': [9904312, 3448737, 2890451, 2466052],\n",
    "    '2010': [9631482, 3393191, 2632035, 2431774],\n",
    "    '2005': [9762546, 3512547, 2517680, 2456016],\n",
    "    '2000': [9853972, 3655437, 2466338, 2473990]\n",
    "}\n",
    "index = ['서울', '부산', '인천', '대구']\n",
    "columns = ['지역', '2015', '2010', '2005', '2000']\n",
    "\n",
    "df = pd.DataFrame(data, index=index, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>지역</th>\n",
       "      <th>2015</th>\n",
       "      <th>2010</th>\n",
       "      <th>2005</th>\n",
       "      <th>2000</th>\n",
       "      <th>2010-2015 증가율</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>서울</th>\n",
       "      <td>수도권</td>\n",
       "      <td>9904312</td>\n",
       "      <td>9631482</td>\n",
       "      <td>9762546</td>\n",
       "      <td>9853972</td>\n",
       "      <td>0.0283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>부산</th>\n",
       "      <td>경상권</td>\n",
       "      <td>3448737</td>\n",
       "      <td>3393191</td>\n",
       "      <td>3512547</td>\n",
       "      <td>3655437</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인천</th>\n",
       "      <td>수도권</td>\n",
       "      <td>2890451</td>\n",
       "      <td>2632035</td>\n",
       "      <td>2517680</td>\n",
       "      <td>2466338</td>\n",
       "      <td>0.0982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>대구</th>\n",
       "      <td>경상권</td>\n",
       "      <td>2466052</td>\n",
       "      <td>2431774</td>\n",
       "      <td>2456016</td>\n",
       "      <td>2473990</td>\n",
       "      <td>0.0141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     지역     2015     2010     2005     2000  2010-2015 증가율\n",
       "서울  수도권  9904312  9631482  9762546  9853972         0.0283\n",
       "부산  경상권  3448737  3393191  3512547  3655437         0.0164\n",
       "인천  수도권  2890451  2632035  2517680  2466338         0.0982\n",
       "대구  경상권  2466052  2431774  2456016  2473990         0.0141"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['2010-2015 증가율'] = round((df['2015'] - df['2010']) / df['2010'], 4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 타이타닉호 승객에 대해서 다음을 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 성별(sex) 인원수, 선실별(class) 인원수, 사망/생존(alive) 인원수를 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "female    314\n",
       "male      577\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.groupby('sex').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "First     216\n",
       "Second    184\n",
       "Third     491\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alive\n",
       "no     549\n",
       "yes    342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.groupby('alive').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) '미성년자', '청년', '중년', '장년', '노년' 승객의 비율을 구하시오. <br>\n",
    "    단, 나이의 기준은 [1, 15, 30, 45, 60, 90] 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 15, 30, 45, 60, 90]\n",
    "labels = ['미성년자', '청년', '중년', '장년', '노년']\n",
    "titanic['age_group'] = pd.cut(titanic.age, bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "미성년자    0.10\n",
       "청년      0.47\n",
       "중년      0.29\n",
       "장년      0.12\n",
       "노년      0.03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(titanic.groupby('age_group').size() / titanic.age_group.count(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 팁 데이터에 대해서 다음을 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 팁의 비율(단위 %)을 소숫점 2째자리까지 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "      <th>tip_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip     sex smoker   day    time  size  tip_pct\n",
       "239       29.03  5.92    Male     No   Sat  Dinner     3     0.20\n",
       "240       27.18  2.00  Female    Yes   Sat  Dinner     2     0.07\n",
       "241       22.67  2.00    Male    Yes   Sat  Dinner     2     0.09\n",
       "242       17.82  1.75    Male     No   Sat  Dinner     2     0.10\n",
       "243       18.78  3.00  Female     No  Thur  Dinner     2     0.16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips['tip_pct'] = round(tips['tip'] / tips['total_bill'], 2)\n",
    "tips.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 팁의 비율이 가장 높은 날은 목, 금, 토, 일요일 중 어떤 날인지 피봇 테이블을 이용하여 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tip_pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fri</th>\n",
       "      <td>0.169474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tip_pct\n",
       "day          \n",
       "Fri  0.169474"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_tip = tips.pivot_table('tip_pct', 'day')\n",
    "day_tip[day_tip.values == day_tip.values.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 다음의 지시대로 SQLite3를 이용하는 파이썬 프로그램을 작성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./test.db') # 파일 DB 접속(일회성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 필드로 [백넘버(PK), 이름, 포지션]을 갖는 테이블 Eagles를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS Eagles \\\n",
    "            (back_no INT NOT NULL, \\\n",
    "             name TEXT, \\\n",
    "             position TEXT, \\\n",
    "             PRIMARY KEY(back_no));')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 1)번에서 만든 테이블에 (8, 정근우, 내야수)를 포함하여 임의로 5명의 선수를 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = ((8, '정근우', '내야수'), (22, '이태양', '투수'), (13, '최재훈', '포수'),\n",
    "          (19, '신은총', '외야수'), (4, '박진원', '투수'))\n",
    "\n",
    "cur = conn.cursor()\n",
    "sql = 'INSERT INTO Eagles VALUES (?, ?, ?);'\n",
    "cur.executemany(sql, players)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 2)번에서 입력한 5명의 선수 모두를 보여주는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, '정근우', '내야수'),\n",
       " (22, '이태양', '투수'),\n",
       " (13, '최재훈', '포수'),\n",
       " (19, '신은총', '외야수'),\n",
       " (4, '박진원', '투수')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('SELECT * FROM Eagles')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 정근우 선수의 포지션을 외야수로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = \"외야수\"\n",
    "back_no = 8\n",
    "cur.execute(\"UPDATE Eagles SET position=? WHERE back_no=?;\", (position, back_no))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 5명의 선수중 백넘버가 가장 큰 선수를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('SELECT max(back_no) FROM Eagles')\n",
    "back_no = cur.fetchone()\n",
    "back_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"DELETE FROM Eagles WHERE back_no=?;\", (back_no))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 시그모이드 함수 (sigmoid(x) = 1 / (1 + np.exp(-x)))와 시그모이드 함수를 미분한 함수 (sigmoid(x) * (1 - sigmoid(x))의 그래프를 그리시오.\n",
    "#### (단, x의 범위는 -3에서 +3까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-3, 3, 0.01)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "p_sigmoid = sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, sigmoid)\n",
    "plt.plot(x, p_sigmoid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 다음 빈 칸에 들어갈 말은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 밥그릇 모양의 그래프에서 경사도를 따라 움직이면서 기울기가 0이 되는 지점을 찾는 것을 (_____)이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 경사하강법'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Answer: 경사하강법\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 1)에서 이동 거리를 정해 주는 것은 (_____) 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Learning rate'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Answer: Learning rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 인공지능에서 신경망을 이루는 가장 중요한 기본 단위는 (______)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 퍼셉트론'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Answer: 퍼셉트론\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 활성화 함수로서 x가 음수일 경우에는 0, x가 0 이상일 때는 x값을 갖는 함수는 (____)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Relu'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Answer: Relu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 딥 러닝에서 입력층과 출력층 사이의 층을 (_____)이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 은닉층'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Answer: 은닉층\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 아이리스 데이터 셋을 이용하여 다음을 구하는 프로그램을 작성하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아이리스 데이터의 4가지 속성(꽃받침 길이/폭, 꽃잎 길이/폭)을 이용하여 품종을 예측\n",
    "- 단, 정확도는 98% 이상일 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iris.sample(frac=1)\n",
    "dataset = sample.values\n",
    "\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LabelEncoder()\n",
    "Y_label = e.fit_transform(Y)\n",
    "\n",
    "Y_encoded = np_utils.to_categorical(Y_label)\n",
    "\n",
    "# Train set, Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_encoded, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 256)               1280      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 104,195\n",
      "Trainable params: 104,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.9555 - acc: 0.4857\n",
      "Epoch 2/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.7275 - acc: 0.7619\n",
      "Epoch 3/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.5696 - acc: 0.6667\n",
      "Epoch 4/200\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.4501 - acc: 0.8095\n",
      "Epoch 5/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.4140 - acc: 0.8095\n",
      "Epoch 6/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.3391 - acc: 0.9524\n",
      "Epoch 7/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.3019 - acc: 0.9048\n",
      "Epoch 8/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.2386 - acc: 0.9619\n",
      "Epoch 9/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.2416 - acc: 0.8952\n",
      "Epoch 10/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.1860 - acc: 0.9524\n",
      "Epoch 11/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1594 - acc: 0.9429\n",
      "Epoch 12/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1273 - acc: 0.9810\n",
      "Epoch 13/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1186 - acc: 0.9619\n",
      "Epoch 14/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1617 - acc: 0.9238\n",
      "Epoch 15/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.1211 - acc: 0.9619\n",
      "Epoch 16/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0993 - acc: 0.9810\n",
      "Epoch 17/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0956 - acc: 0.9714\n",
      "Epoch 18/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1005 - acc: 0.9429\n",
      "Epoch 19/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.2311 - acc: 0.9143\n",
      "Epoch 20/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.2298 - acc: 0.9143\n",
      "Epoch 21/200\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0855 - acc: 0.950 - 0s 114us/step - loss: 0.1452 - acc: 0.9143\n",
      "Epoch 22/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.1054 - acc: 0.9524\n",
      "Epoch 23/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1129 - acc: 0.9524\n",
      "Epoch 24/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1211 - acc: 0.9524\n",
      "Epoch 25/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0964 - acc: 0.9619\n",
      "Epoch 26/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0902 - acc: 0.9714\n",
      "Epoch 27/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0893 - acc: 0.9714\n",
      "Epoch 28/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0905 - acc: 0.9714\n",
      "Epoch 29/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0964 - acc: 0.9524\n",
      "Epoch 30/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1087 - acc: 0.9524\n",
      "Epoch 31/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0969 - acc: 0.9524\n",
      "Epoch 32/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0801 - acc: 0.9810\n",
      "Epoch 33/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1072 - acc: 0.9524\n",
      "Epoch 34/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1128 - acc: 0.9429\n",
      "Epoch 35/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.2174 - acc: 0.9238\n",
      "Epoch 36/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1590 - acc: 0.9238\n",
      "Epoch 37/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1168 - acc: 0.9524\n",
      "Epoch 38/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1550 - acc: 0.9429\n",
      "Epoch 39/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1111 - acc: 0.9429\n",
      "Epoch 40/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1067 - acc: 0.9619\n",
      "Epoch 41/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1476 - acc: 0.9333\n",
      "Epoch 42/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1032 - acc: 0.9619\n",
      "Epoch 43/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0822 - acc: 0.9524\n",
      "Epoch 44/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0917 - acc: 0.9619\n",
      "Epoch 45/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0745 - acc: 0.9714\n",
      "Epoch 46/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1047 - acc: 0.9524\n",
      "Epoch 47/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0838 - acc: 0.9714\n",
      "Epoch 48/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0799 - acc: 0.9810\n",
      "Epoch 49/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0757 - acc: 0.9810\n",
      "Epoch 50/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1055 - acc: 0.9619\n",
      "Epoch 51/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0741 - acc: 0.9810\n",
      "Epoch 52/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0764 - acc: 0.9810\n",
      "Epoch 53/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1159 - acc: 0.9524\n",
      "Epoch 54/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0809 - acc: 0.9714\n",
      "Epoch 55/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1220 - acc: 0.9619\n",
      "Epoch 56/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0787 - acc: 0.9714\n",
      "Epoch 57/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0992 - acc: 0.9524\n",
      "Epoch 58/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1090 - acc: 0.9714\n",
      "Epoch 59/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0950 - acc: 0.9619\n",
      "Epoch 60/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0921 - acc: 0.9714\n",
      "Epoch 61/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0930 - acc: 0.9619\n",
      "Epoch 62/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1136 - acc: 0.9524\n",
      "Epoch 63/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0931 - acc: 0.9524\n",
      "Epoch 64/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0829 - acc: 0.9619\n",
      "Epoch 65/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0698 - acc: 0.9810\n",
      "Epoch 66/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0870 - acc: 0.9524\n",
      "Epoch 67/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0936 - acc: 0.9619\n",
      "Epoch 68/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0743 - acc: 0.9810\n",
      "Epoch 69/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0780 - acc: 0.9714\n",
      "Epoch 70/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0691 - acc: 0.9714\n",
      "Epoch 71/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0703 - acc: 0.9619\n",
      "Epoch 72/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0705 - acc: 0.9714\n",
      "Epoch 73/200\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.1036 - acc: 0.9524\n",
      "Epoch 74/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0672 - acc: 0.9905\n",
      "Epoch 75/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0699 - acc: 0.9714\n",
      "Epoch 76/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0778 - acc: 0.9714\n",
      "Epoch 77/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0666 - acc: 0.9810\n",
      "Epoch 78/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0900 - acc: 0.9619\n",
      "Epoch 79/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.1502 - acc: 0.9429\n",
      "Epoch 80/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0804 - acc: 0.9619\n",
      "Epoch 81/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0665 - acc: 0.9810\n",
      "Epoch 82/200\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0732 - acc: 0.9714\n",
      "Epoch 83/200\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0769 - acc: 0.9619\n",
      "Epoch 84/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0793 - acc: 0.9714\n",
      "Epoch 85/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0649 - acc: 0.9810\n",
      "Epoch 86/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.1161 - acc: 0.9524\n",
      "Epoch 87/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0857 - acc: 0.9714\n",
      "Epoch 88/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0690 - acc: 0.9714\n",
      "Epoch 89/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0960 - acc: 0.9619\n",
      "Epoch 90/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1833 - acc: 0.9333\n",
      "Epoch 91/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1163 - acc: 0.9524\n",
      "Epoch 92/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0938 - acc: 0.9619\n",
      "Epoch 93/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0798 - acc: 0.9714\n",
      "Epoch 94/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0750 - acc: 0.9714\n",
      "Epoch 95/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0769 - acc: 0.9810\n",
      "Epoch 96/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1126 - acc: 0.9524\n",
      "Epoch 97/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0711 - acc: 0.9714\n",
      "Epoch 98/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0909 - acc: 0.9524\n",
      "Epoch 99/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1789 - acc: 0.9333\n",
      "Epoch 100/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1592 - acc: 0.9333\n",
      "Epoch 101/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1031 - acc: 0.9429\n",
      "Epoch 102/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1048 - acc: 0.9524\n",
      "Epoch 103/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0934 - acc: 0.9619\n",
      "Epoch 104/200\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0539 - acc: 1.000 - 0s 124us/step - loss: 0.0855 - acc: 0.9714\n",
      "Epoch 105/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0780 - acc: 0.9714\n",
      "Epoch 106/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0855 - acc: 0.9619\n",
      "Epoch 107/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0847 - acc: 0.9714\n",
      "Epoch 108/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0782 - acc: 0.9714\n",
      "Epoch 109/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0678 - acc: 0.9714\n",
      "Epoch 110/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0657 - acc: 0.9714\n",
      "Epoch 111/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0679 - acc: 0.9810\n",
      "Epoch 112/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0658 - acc: 0.9714\n",
      "Epoch 113/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0840 - acc: 0.9524\n",
      "Epoch 114/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0843 - acc: 0.9714\n",
      "Epoch 115/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0742 - acc: 0.9714\n",
      "Epoch 116/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0638 - acc: 0.9810\n",
      "Epoch 117/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0644 - acc: 0.9714\n",
      "Epoch 118/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0769 - acc: 0.9619\n",
      "Epoch 119/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1349 - acc: 0.9524\n",
      "Epoch 120/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0967 - acc: 0.9524\n",
      "Epoch 121/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1003 - acc: 0.9429\n",
      "Epoch 122/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0901 - acc: 0.9619\n",
      "Epoch 123/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0717 - acc: 0.9619\n",
      "Epoch 124/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0961 - acc: 0.9524\n",
      "Epoch 125/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0643 - acc: 0.9810\n",
      "Epoch 126/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0888 - acc: 0.9619\n",
      "Epoch 127/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0718 - acc: 0.9810\n",
      "Epoch 128/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0823 - acc: 0.9619\n",
      "Epoch 129/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1227 - acc: 0.9429\n",
      "Epoch 130/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0754 - acc: 0.9714\n",
      "Epoch 131/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0838 - acc: 0.9429\n",
      "Epoch 132/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0912 - acc: 0.9524\n",
      "Epoch 133/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0767 - acc: 0.9714\n",
      "Epoch 134/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0646 - acc: 0.9810\n",
      "Epoch 135/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0682 - acc: 0.9810\n",
      "Epoch 136/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0671 - acc: 0.9714\n",
      "Epoch 137/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0616 - acc: 0.9905\n",
      "Epoch 138/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0626 - acc: 0.9810\n",
      "Epoch 139/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0687 - acc: 0.9714\n",
      "Epoch 140/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0712 - acc: 0.9714\n",
      "Epoch 141/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0596 - acc: 0.9810\n",
      "Epoch 142/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0614 - acc: 0.9810\n",
      "Epoch 143/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0684 - acc: 0.9810\n",
      "Epoch 144/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0615 - acc: 0.9810\n",
      "Epoch 145/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0657 - acc: 0.9810\n",
      "Epoch 146/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0689 - acc: 0.9714\n",
      "Epoch 147/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1018 - acc: 0.9524\n",
      "Epoch 148/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0815 - acc: 0.9619\n",
      "Epoch 149/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0816 - acc: 0.9714\n",
      "Epoch 150/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0962 - acc: 0.9524\n",
      "Epoch 151/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0786 - acc: 0.9714\n",
      "Epoch 152/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0652 - acc: 0.9714\n",
      "Epoch 153/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0745 - acc: 0.9619\n",
      "Epoch 154/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0659 - acc: 0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0719 - acc: 0.9619\n",
      "Epoch 156/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0814 - acc: 0.9619\n",
      "Epoch 157/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0707 - acc: 0.9810\n",
      "Epoch 158/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0682 - acc: 0.9810\n",
      "Epoch 159/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0696 - acc: 0.9714\n",
      "Epoch 160/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0642 - acc: 0.9810\n",
      "Epoch 161/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0597 - acc: 0.9810\n",
      "Epoch 162/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0804 - acc: 0.9714\n",
      "Epoch 163/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0575 - acc: 0.9810\n",
      "Epoch 164/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0728 - acc: 0.9714\n",
      "Epoch 165/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0656 - acc: 0.9714\n",
      "Epoch 166/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0590 - acc: 0.9810\n",
      "Epoch 167/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0843 - acc: 0.9714\n",
      "Epoch 168/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0614 - acc: 0.9810\n",
      "Epoch 169/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0937 - acc: 0.9619\n",
      "Epoch 170/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1354 - acc: 0.9524\n",
      "Epoch 171/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1591 - acc: 0.9429\n",
      "Epoch 172/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1078 - acc: 0.9714\n",
      "Epoch 173/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1245 - acc: 0.9429\n",
      "Epoch 174/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0660 - acc: 0.9810\n",
      "Epoch 175/200\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.1094 - acc: 0.9429\n",
      "Epoch 176/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1001 - acc: 0.9619\n",
      "Epoch 177/200\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0988 - acc: 0.9524\n",
      "Epoch 178/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1259 - acc: 0.9714\n",
      "Epoch 179/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.1256 - acc: 0.9429\n",
      "Epoch 180/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.1472 - acc: 0.9429\n",
      "Epoch 181/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0761 - acc: 0.9714\n",
      "Epoch 182/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.1006 - acc: 0.9429\n",
      "Epoch 183/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0830 - acc: 0.9619\n",
      "Epoch 184/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0890 - acc: 0.9619\n",
      "Epoch 185/200\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0671 - acc: 0.9810\n",
      "Epoch 186/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0707 - acc: 0.9714\n",
      "Epoch 187/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0925 - acc: 0.9714\n",
      "Epoch 188/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0739 - acc: 0.9619\n",
      "Epoch 189/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0841 - acc: 0.9524\n",
      "Epoch 190/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0617 - acc: 0.9714\n",
      "Epoch 191/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0728 - acc: 0.9714\n",
      "Epoch 192/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0877 - acc: 0.9524\n",
      "Epoch 193/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0600 - acc: 0.9714\n",
      "Epoch 194/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0770 - acc: 0.9714\n",
      "Epoch 195/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0674 - acc: 0.9714\n",
      "Epoch 196/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0620 - acc: 0.9810\n",
      "Epoch 197/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0641 - acc: 0.9714\n",
      "Epoch 198/200\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0631 - acc: 0.9619\n",
      "Epoch 199/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0617 - acc: 0.9810\n",
      "Epoch 200/200\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0608 - acc: 0.9714\n",
      "105/105 [==============================] - 0s 2ms/step\n",
      "\n",
      " Accuracy: 0.9810\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=4, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 실행\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=20)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_train, y_train)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 413us/step\n",
      "[ loss: 0.0268, acc: 1.0000 ]\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print('[ loss: %.4f, acc: %.4f ]' % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Idx:21] [1, 1]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0, len(X_test))\n",
    "\n",
    "predictions = model.predict(X_test[n].reshape(1,4))\n",
    "print(\"[Idx:%d] [%d, %d]\" % (n, np.argmax(predictions), list(y_test[n]).index(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 피마 인디언 데이터 셋을 이용하여 다음을 구하는 프로그램을 작성하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 피마 인디언 데이터의 8가지 속성을 이용하여 당뇨병 여부를 판단할 것\n",
    "- 단, 데이터의 25% 테스트 데이터로 사용하여 정확도를 구할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\workspace-Jwp\\python\\Data_Science\\deeplearning\\deeplearning\\dataset\\pima-indians-diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima = pd.read_csv(path, header=None)\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pima.sample(frac=1)\n",
    "dataset = sample.values\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_239 (Dense)            (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 2s 4ms/step - loss: 3.7459 - acc: 0.6389\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 130us/step - loss: 2.9560 - acc: 0.5851\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 2.3541 - acc: 0.5434\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 1.1341 - acc: 0.5556\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 0.8486 - acc: 0.6094\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.7803 - acc: 0.6389\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.7919 - acc: 0.6146\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 0.7116 - acc: 0.6372\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 163us/step - loss: 0.6934 - acc: 0.6458\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.6640 - acc: 0.6406\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.6700 - acc: 0.6632\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.6547 - acc: 0.6632\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.6451 - acc: 0.6563\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.6634 - acc: 0.6337\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.6451 - acc: 0.6580\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.6523 - acc: 0.6563\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.6537 - acc: 0.6632\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.6283 - acc: 0.6597\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 0.6375 - acc: 0.6667\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 172us/step - loss: 0.6367 - acc: 0.6580\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.6366 - acc: 0.6736\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.6195 - acc: 0.6632\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.6398 - acc: 0.6701\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.6342 - acc: 0.6736\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.6201 - acc: 0.6719\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.6097 - acc: 0.6684\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 128us/step - loss: 0.6075 - acc: 0.6788\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.6099 - acc: 0.6788\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.6224 - acc: 0.6615\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.6069 - acc: 0.6753\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.5950 - acc: 0.6910\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5965 - acc: 0.6927\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 0.6118 - acc: 0.6701\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 0.6016 - acc: 0.6823\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.6079 - acc: 0.6719\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.6110 - acc: 0.6875\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.6182 - acc: 0.6771\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5922 - acc: 0.6944\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.6010 - acc: 0.6701\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5939 - acc: 0.6858\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5902 - acc: 0.6736\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5915 - acc: 0.6875\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5820 - acc: 0.6806\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5893 - acc: 0.6840\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5825 - acc: 0.6858\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5839 - acc: 0.6858\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5837 - acc: 0.6910\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5946 - acc: 0.6736\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 130us/step - loss: 0.6558 - acc: 0.6806\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5941 - acc: 0.6684\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5837 - acc: 0.6788\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.5814 - acc: 0.6979\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 163us/step - loss: 0.5690 - acc: 0.6892\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5976 - acc: 0.6892\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5783 - acc: 0.6806\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5789 - acc: 0.6927\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5920 - acc: 0.6823\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 0.5907 - acc: 0.6788\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 0.5732 - acc: 0.6823\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5753 - acc: 0.6858\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5768 - acc: 0.6910\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5611 - acc: 0.6927\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5756 - acc: 0.6910\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5696 - acc: 0.6962\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5610 - acc: 0.6997\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5546 - acc: 0.6944\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5634 - acc: 0.6927\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.6037 - acc: 0.6858\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5881 - acc: 0.6823\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5640 - acc: 0.7153\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5513 - acc: 0.7083\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5756 - acc: 0.6927\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5546 - acc: 0.6997\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5567 - acc: 0.7049\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5765 - acc: 0.6910\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 139us/step - loss: 0.5588 - acc: 0.7135\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5670 - acc: 0.7101\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5468 - acc: 0.7066\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5644 - acc: 0.6997\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5587 - acc: 0.6997\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5500 - acc: 0.7066\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5531 - acc: 0.7101\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5453 - acc: 0.7240\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5594 - acc: 0.6962\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5497 - acc: 0.7274\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.6011 - acc: 0.7170\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5885 - acc: 0.6927\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5530 - acc: 0.7083\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5453 - acc: 0.7135\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5421 - acc: 0.7101\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5476 - acc: 0.7083\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5466 - acc: 0.7066\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5458 - acc: 0.7101\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5411 - acc: 0.7083\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5395 - acc: 0.7170\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5387 - acc: 0.7257\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5491 - acc: 0.7135\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5444 - acc: 0.7135\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5298 - acc: 0.7188\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.5641 - acc: 0.7170\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.6113 - acc: 0.6892\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5341 - acc: 0.7222\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5355 - acc: 0.7222\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5404 - acc: 0.7292\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5391 - acc: 0.7222\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5445 - acc: 0.7188\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5266 - acc: 0.7170\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5369 - acc: 0.7205\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5344 - acc: 0.7240\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5417 - acc: 0.7170\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5283 - acc: 0.7257\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5219 - acc: 0.7222\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5242 - acc: 0.7274\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5380 - acc: 0.7153\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5330 - acc: 0.7240\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5302 - acc: 0.7257\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5358 - acc: 0.7448\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5247 - acc: 0.7240\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5348 - acc: 0.7170\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 0.5211 - acc: 0.7396\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5157 - acc: 0.7396\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5247 - acc: 0.7361\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5303 - acc: 0.7378\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5187 - acc: 0.7274\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 0.5238 - acc: 0.7170\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5280 - acc: 0.7361\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5262 - acc: 0.7413\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5243 - acc: 0.7396\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5198 - acc: 0.7309\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5149 - acc: 0.7361\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5271 - acc: 0.7292\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5223 - acc: 0.7222\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5156 - acc: 0.7378\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5197 - acc: 0.7292\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5120 - acc: 0.7326\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5074 - acc: 0.7483\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5299 - acc: 0.7344\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5266 - acc: 0.7222\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5281 - acc: 0.7188\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5361 - acc: 0.7292\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5420 - acc: 0.7309\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5205 - acc: 0.7361\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5199 - acc: 0.7240\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 130us/step - loss: 0.5107 - acc: 0.7465\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5043 - acc: 0.7396\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5169 - acc: 0.7326\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5071 - acc: 0.7500\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5079 - acc: 0.7326\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5205 - acc: 0.7205\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5235 - acc: 0.7326\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.5117 - acc: 0.7431\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5007 - acc: 0.7448\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5018 - acc: 0.7326\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5169 - acc: 0.7344\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5067 - acc: 0.7378\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5364 - acc: 0.7378\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5159 - acc: 0.7344\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 142us/step - loss: 0.5166 - acc: 0.7240\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5073 - acc: 0.7309\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.4949 - acc: 0.7378\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5384 - acc: 0.7222\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5111 - acc: 0.7465\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5038 - acc: 0.7378\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5026 - acc: 0.7483\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.5163 - acc: 0.7483\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 0.5287 - acc: 0.7465\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.4968 - acc: 0.7361\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5182 - acc: 0.7361\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.5020 - acc: 0.7431\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5107 - acc: 0.7257\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5345 - acc: 0.7378\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5447 - acc: 0.7257\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5520 - acc: 0.7309\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5096 - acc: 0.7361\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.5028 - acc: 0.7483\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.4966 - acc: 0.7483\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.4967 - acc: 0.7431\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.4938 - acc: 0.7552\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5101 - acc: 0.7448\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.5077 - acc: 0.7483\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5003 - acc: 0.7378\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.4902 - acc: 0.7587\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5187 - acc: 0.7326\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.4939 - acc: 0.7448\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.4922 - acc: 0.7569\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5115 - acc: 0.7326\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.4886 - acc: 0.7378\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5037 - acc: 0.7344\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.4962 - acc: 0.7517\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.4955 - acc: 0.7396\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.4923 - acc: 0.7552\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.4876 - acc: 0.7413\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.4864 - acc: 0.7569\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.4911 - acc: 0.7396\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.5013 - acc: 0.7378\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.4927 - acc: 0.7517\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.5170 - acc: 0.7240\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 0.5021 - acc: 0.7413\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 0.4928 - acc: 0.7483\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 0.4939 - acc: 0.7552\n",
      "576/576 [==============================] - 1s 1ms/step\n",
      "\n",
      " Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 실행\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=10)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_train, y_train)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 81us/step\n",
      "[ loss: 0.6760, acc: 0.7292 ]\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print('[ loss: %.4f, acc: %.4f ]' % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Idx:24] [0, 1]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0, len(X_test))\n",
    "\n",
    "predictions = model.predict(X_test[n].reshape(1,8))\n",
    "print(\"[Idx:%d] [%d, %d]\" % (n, (predictions+0.1).round(), y_test[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 과제 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ML",
   "language": "python",
   "name": "new_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
